{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upwork project summary from client\n",
    "My daughter day care has an app where they post photos everyday but they need to be downloaded one at a time. Seeing if there is a way to write a script to download all media from the app. Would be great to capture these memories\n",
    "\n",
    "App is called “my bright day” likely 1000 plus photos and video\n",
    "\n",
    "- download both video and pictures (high resolution version not thumbnail)\n",
    "- Ideally have photo have dates in file_name so can be sorted by time\n",
    "- Upload to a Google Drive folder and share\n",
    "- ideally provide instructions so i can repeat the process later for future photos.\n",
    "\n",
    "\n",
    "## How to setup\n",
    "This notebook is written to provide the solution for the porject. To run this notebook, you should have python **3.10** installed on local machine, no extra python package is needed. To run this notebook, I recommend using VSCode (download VS Code from [here](https://code.visualstudio.com/download)). Of course if you need some help or want to run it another way, can message and we can setup a consultation session.\n",
    "\n",
    "## How this notebook is organized\n",
    "This notebook groups each functional block in steps with documents. Just follow the instruction on each step and run the code.\n",
    "\n",
    "## Technical details\n",
    "Photos and videos are stored in Google Cloud storage can be retrieved by sending a request to the **signed URL**(basically GCS URL with a time sensitive token). To get signed URL, **my bright day's a media API** is exposed that takes the media ID and my bright day's authentication key. The media IDs can be retrieved by **my bright day's snapshot API** that contains all the media information for a given date range. With this knowledge, the script offers 2 use cases \n",
    " - Download all the medias all the way to the earlier date (ths project ask)\n",
    " - Download media for a particular date range.\n",
    "\n",
    "You can specify the download destination root folder. Within this folder, the script creates sub folder using \"YYYY-mm-dd\" format. The media is named after its capture time. Currently only **jpeg** and **mp4** format are supoorted.\n",
    "The download process creates a report that conatains all the metadata (media name, download success/fail, fail reason, etc) for the download.\n",
    "\n",
    "The script uses multi thread to speed up download. Actual number of work is dependent to the number of cores of your environment.\n",
    "\n",
    "In order for the script to authenticate. You need to login to My bright app from laptop and get the api key. Instruction is provided in the below section close to the actual code.\n",
    "\n",
    "If there is any question, please let me know so we can schedule a consultation session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Execute the below cell \n",
    "It'll make all the functions available for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def get_dependent_from_guardian(api_key, guardian_id):\n",
    "    \"\"\"\n",
    "    Get dependent ID from the guardian ID.\n",
    "    \"\"\"\n",
    "    dependent_url = f\"https://mybrightday.brighthorizons.com/api/v2/dependents/guardian/{guardian_id}\"\n",
    "    header={\"x-api-key\" : api_key}\n",
    "    response = requests.get(dependent_url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()[0]['id']\n",
    "\n",
    "\n",
    "def get_video_from_signed_url(signed_url, video_name, save_path):\n",
    "    response = requests.get(signed_url)\n",
    "    with open(os.path.join(save_path, video_name), \"wb\") as f_out:\n",
    "          f_out.write(response.content)\n",
    "    \n",
    "def get_image_from_signed_url(signed_url, image_name, save_path, save=True):\n",
    "    response = requests.get(signed_url)\n",
    "    response.raise_for_status()\n",
    "    im = BytesIO(response.content)\n",
    "    img = Image.open(im)\n",
    "    img = img.convert('RGB')\n",
    "    if save:\n",
    "        img.save(os.path.join(save_path, image_name))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_media_metadata_from_media_id(api_key, media_id):\n",
    "    mbd_url = f\"https://mybrightday.brighthorizons.com/api/v2/media/{media_id}\"\n",
    "    header={\"x-api-key\" : api_key}\n",
    "    response = requests.get(mbd_url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    main_image_url = response.json()['signed_url']\n",
    "    media_type = response.json()['mime_type']\n",
    "    return main_image_url, media_type\n",
    "\n",
    "def get_daily_report(api_key, dependent_id, start_date, end_date=None):\n",
    "    if not end_date:\n",
    "        end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "    daily_report_url = f\"https://mybrightday.brighthorizons.com/api/v2/dependent/{dependent_id}/daily_reports?start={start_date}&end={end_date}\"\n",
    "    header={\"x-api-key\" : api_key}\n",
    "    response = requests.get(daily_report_url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def get_media_for_days(api_key, days, base_path):\n",
    "    \"\"\"\n",
    "    days: a list of metadata from get_daily_report\n",
    "    \"\"\"\n",
    "    all_report = []\n",
    "    for day in days:\n",
    "        date = day['for_date']\n",
    "        #os.makedirs(base_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(base_path, date), exist_ok=True)\n",
    "\n",
    "        snapshots = day['snapshot_entries']\n",
    "        report = []\n",
    "        for snapshot in snapshots:\n",
    "            media_id = snapshot['attachment_id']\n",
    "            capture_time = snapshot['capture_time']\n",
    "            report_snapshot = {\"media_id\":media_id, \"capture_time\": capture_time}\n",
    "            try:\n",
    "                media_url, media_type = get_media_metadata_from_media_id(api_key, media_id)\n",
    "                report_snapshot[\"media_type\"] = media_type\n",
    "            except:\n",
    "                report_snapshot['state']='failed'\n",
    "                report_snapshot['reason']='cannot get media metadata'\n",
    "                continue\n",
    "\n",
    "            if media_type == \"image/jpeg\":\n",
    "                try:\n",
    "                    get_image_from_signed_url(media_url, f\"{capture_time}.jpg\", os.path.join(base_path, date), True)\n",
    "                    report_snapshot['state']='success'\n",
    "                except Exception as exc:\n",
    "                    report_snapshot['state']='failed'\n",
    "                    report_snapshot['reason']= exc\n",
    "            elif media_type == \"video/mp4\":\n",
    "                try:\n",
    "                    get_video_from_signed_url(media_url, f\"{capture_time}.mp4\", os.path.join(base_path, date))\n",
    "                    report_snapshot['state']='success'\n",
    "                except Exception as exc:\n",
    "                    report_snapshot['state']='failed'\n",
    "                    report_snapshot['reason']= exc\n",
    "            else:\n",
    "                report_snapshot['state']='failed'\n",
    "                report_snapshot['reason']='unsupported media type'\n",
    "            report.append(report_snapshot)\n",
    "        all_report.extend(report)\n",
    "    return all_report\n",
    "\n",
    "def get_media_for_days_m(api_key, days, base_path):\n",
    "    all_report_m = []\n",
    "    worker_count = min(32, os.cpu_count() + 4)\n",
    "    chunked_days = np.array_split(days, worker_count)\n",
    "    with ThreadPoolExecutor(max_workers=worker_count) as executor:\n",
    "        future_to_days = {executor.submit(get_media_for_days, api_key, days, base_path): days for days in chunked_days}\n",
    "        for future in as_completed(future_to_days):\n",
    "            try:\n",
    "                report = future.result()\n",
    "                all_report_m.extend(report)\n",
    "            except Exception as exc:\n",
    "                print(f'Encountered an exception: {exc}')\n",
    "    return all_report_m\n",
    "\n",
    "def get_stats_on_report(all_report):\n",
    "    success_count, failed_count=0,0\n",
    "    for report in all_report:\n",
    "        if report['state'] == \"success\":\n",
    "            success_count+=1 \n",
    "        else:\n",
    "            failed_count += 1\n",
    "    print(f\"A total of {success_count} downloads are successful, and {failed_count} downloads are failed.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Login to My Bright Day website from Chrome on your laptop (not cellphone)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find your guardian ID from the URL\n",
    "The URL should look like:\n",
    "\n",
    "https://mybrightdayapp.brighthorizons.com/home/{guardian_id}/activity-feed\n",
    "\n",
    "The ID between home and activity-feed is your guardian ID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get the api key\n",
    "- While you are on My bright day home page, right click and click inspect to bring up the developer tools. \n",
    "- Navigate to Network tab.\n",
    "- In the Filter, type selections.\n",
    "- On the bottom right panel, under Headers tab - Request Headers, there is a key called X-Api-Key. \n",
    "- Check this image for example\n",
    "![image](get_api_key.png)\n",
    "Set the X-Api-Key to `api_key` and guadian ID to `guardian_ID` in the below cell. Set save directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key= \"15b037bc-799c-4558-ae3f-35a30972e758\"\n",
    "guardian_id = \"6214d4027193ddd5b1c937c2\"\n",
    "\n",
    "save_directory = \"all_media_12\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now start to download the media!\n",
    "Assuming you have one child. The below script will download the child's all the media generated between `last_month`, which is by default set to `today` and `months_lookback` months. Execute the cell and wait patiently. This cell is tend to run long. To give you an idea, 1000 media takes 6m on a M1 macbook pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get daily snapshot between 2023-06-11 and 2023-07-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-06-11&end=2023-07-11\n",
      "get daily snapshot between 2023-05-11 and 2023-06-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-05-11&end=2023-06-11\n",
      "get daily snapshot between 2023-04-11 and 2023-05-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-04-11&end=2023-05-11\n",
      "Encountered an exception: [Errno 17] File exists: 'save_directory/2023-05-11'\n",
      "get daily snapshot between 2023-03-11 and 2023-04-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-03-11&end=2023-04-11\n",
      "Encountered an exception: [Errno 17] File exists: 'save_directory/2023-04-11'\n",
      "get daily snapshot between 2023-02-11 and 2023-03-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-02-11&end=2023-03-11\n",
      "get daily snapshot between 2023-01-11 and 2023-02-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2023-01-11&end=2023-02-11\n",
      "get daily snapshot between 2022-12-11 and 2023-01-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-12-11&end=2023-01-11\n",
      "Encountered an exception: [Errno 17] File exists: 'save_directory/2023-01-11'\n",
      "get daily snapshot between 2022-11-11 and 2022-12-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-11-11&end=2022-12-11\n",
      "get daily snapshot between 2022-10-11 and 2022-11-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-10-11&end=2022-11-11\n",
      "get daily snapshot between 2022-09-11 and 2022-10-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-09-11&end=2022-10-11\n",
      "Encountered an exception: [Errno 17] File exists: 'save_directory/2022-10-11'\n",
      "get daily snapshot between 2022-08-11 and 2022-09-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-08-11&end=2022-09-11\n",
      "get daily snapshot between 2022-07-11 and 2022-08-11.\n",
      "daily report url:https://mybrightday.brighthorizons.com/api/v2/dependent/62154527be6f1b5ca640916b/daily_reports?start=2022-07-11&end=2022-08-11\n",
      "Encountered an exception: [Errno 17] File exists: 'save_directory/2022-08-11'\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import json \n",
    "\n",
    "child_id = get_dependent_from_guardian(api_key, guardian_id)\n",
    "## Set this month back\n",
    "month_lookback = 12\n",
    "last_month = datetime.today()\n",
    "\n",
    "all_report = []\n",
    "\n",
    "while month_lookback > 0:\n",
    "    this_month = last_month\n",
    "    this_month_str = this_month.strftime('%Y-%m-%d')\n",
    "    last_month= this_month - relativedelta(months=1)\n",
    "    last_month_str = last_month.strftime('%Y-%m-%d')\n",
    "    print(f\"get daily snapshot between {last_month_str} and {this_month_str}.\")\n",
    "    days = get_daily_report(api_key, child_id, last_month_str, this_month_str)\n",
    "    report = get_media_for_days_m(api_key, days, save_directory)\n",
    "    all_report.extend(report)\n",
    "    month_lookback -=1\n",
    "\n",
    "get_stats_on_report(all_report)\n",
    "\n",
    "with open(\"report.json\", \"w\") as outfile:\n",
    "    json.dump(all_report, outfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another option to download\n",
    "This is the one button click approach. It'll down all the media all the way to the beginning of the history. No parameter needs to be set for this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import json \n",
    "\n",
    "child_id = get_dependent_from_guardian(api_key, guardian_id)\n",
    "last_month = datetime.today()\n",
    "\n",
    "all_report = []\n",
    "\n",
    "while True:\n",
    "    this_month = last_month\n",
    "    this_month_str = this_month.strftime('%Y-%m-%d')\n",
    "    last_month= this_month - relativedelta(months=1)\n",
    "    last_month_str = last_month.strftime('%Y-%m-%d')\n",
    "    print(f\"get daily snapshot between {last_month_str} and {this_month_str}.\")\n",
    "    days = get_daily_report(api_key, child_id, last_month_str, this_month_str)\n",
    "    if len(days) == 0:\n",
    "        break\n",
    "    report = get_media_for_days_m(api_key, days, save_directory)\n",
    "    all_report.extend(report)\n",
    "\n",
    "get_stats_on_report(all_report)\n",
    "\n",
    "with open(\"report.json\", \"w\") as outfile:\n",
    "    json.dump(all_report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single threaded execution for debugging\n",
    "\n",
    "api_key= \"15b037bc-799c-4558-ae3f-35a30972e758\"\n",
    "guardian_id = \"6214d4027193ddd5b1c937c2\"\n",
    "child_id = get_dependent_from_guardian(guardian_id)[0]\n",
    "\n",
    "days = get_daily_report(api_key, child_id, start_date=\"2022-06-10\", end_date=\"2022-07-01\")\n",
    "all_report = []\n",
    "for day in days:\n",
    "    date = day['for_date']\n",
    "    report = get_media_for_days(api_key, [day])\n",
    "    all_report.extend(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
